\documentclass{article} % For LaTeX2e
\usepackage{nips11submit_e,times}
%\documentstyle[nips10submit_09,times,art10]{article} % For LaTeX 2.09

\usepackage{amsmath}
\usepackage{bm}
\usepackage{bbm}
\usepackage{verbatim}
\usepackage{nccmath}
\usepackage[psamsfonts]{amssymb}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{cancel}

\title{Variational Gaussian Process Dynamical Systems}


\author{
David S.~Hippocampus\thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213 \\
\texttt{hippo@cs.cranberry-lemon.edu} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
(if needed)\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\newcommand{\highlight}[1]{\colorbox{yellow}{#1}}

\newcommand{\bff}{\mathbf{f}}
\newcommand{\bfu}{\mathbf{u}}
\newcommand{\bfy}{\mathbf{y}}
\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bft}{\mathbf{t}}
\newcommand{\bfk}{\mathbf{k}}
\newcommand{\bfmu}{\boldsymbol \mu}
\newcommand{\bfz}{\mathbf{0}}

\newcommand{\T}{{\top}}

\newcommand{\bfa}{\mathbf{a}}
\newcommand{\bb}{\beta^{-1}}
\newcommand{\la}{\left\langle}
\newcommand{\ra}{\right\rangle}
\newcommand{\vv}{\vartheta}



\maketitle

\begin{abstract}
  High dimensional time series are endemic in applications of machine
  learning such as robotics (sensor data), computational biology (gene
  expression data), vision (video sequences) and graphics (motion
  capture data). Practical nonlinear probabilistic approaches to this
  data are required. In this paper we introduce the variational
  Gaussian process dynamical system. Our work builds on recent
  variational approximations for Gaussian process latent variable
  models to allow for simultaneous nonlinear dimensionality reduction,
  with automatic determination of data dimensionality, alongside
  learning a dynamical prior. We demonstrate our approach on a human motion capture data set and a series of high resolution video sequences.
% %
% The proposed model employs Gaussian processes (GPs) to probabilistically associate the observations with a lower dimensional latent manifold but, unlike standard non-linear dimensionality reduction methods, it does not assume independece across datapoints; instead, it incorporates dynamics via a temporal GP prior on the latent space.
% %
% %\textcolor{green}{The proposed model assumes that observations are generated by a low-dimensional latent space where dynamics are incorporated via a temporal Gaussian process (GP) prior. The mapping back to the observation space is also done using Gaussian processes.}
% %
% In contrast to past approaches, our proposed formulation efficiently integrates the model into a fully Bayesian framework;
% %We show how this model can be trained efficiently in a fully Bayesian manner by employing Gaussian variational approximation. 
% %This training framework gives the model the ability to generalise better, automatically select an effective dimensionality for the latent space as well as learn from high dimensional data. The method is demonstrated on human motion and video sequence data.
% %
% this treatment equips the model with attractive properties, 
% such as the ability to automatically select an effective dimensionality for the latent space and learn the parameters of the dynamical prior, leading to better generalization. We demonstrate the approach on human motion capture data and unprocessed high dimensional video.
% %such as the ability to generalise better, automatically select an effective dimensionality for the latent space as well as handle high dimensional data. The method is demonstrated on human motion and high dimensional video sequence data.
% %
\end{abstract}



\section{Introduction}

Nonlinear probabilistic modeling of high dimensional time series data
is a key challenge for the machine learning community. A standard
approach is to simultaneously apply a nonlinear dimensionality
reduction to the data whilst governing the latent space with a
nonlinear temporal prior. The key difficulty for such approaches is
that analytic marginalization of the latent space is typically
intractable. Markov chain Monte Carlo approaches can also be
problematic as latent trajectories are strongly correlated making
efficient sampling a challenge. One promising approach
%
%
\textcolor{red}{[maybe the reader will think that this is a promising approach to the analytic marginalisation mentioned earlier? ]}
%
%
 has been to
extend the Gaussian process latent variable model
\cite{GPLVM,GPLVM2} with a dynamical
prior for the latent space and seek a maximum a posteriori (MAP)
solution for the latent points
\cite{GPDM,Wang:gpdm08,hgplvm}. \cite{GP-Based} further extend 
these models for fully Bayesian filtering in a robotics setting. We
refer to this class of dynamical models based on the GP-LVM as
Gaussian process dynamical systems (GPDS). However, the use of a MAP
approximation for training these models presents key problems.
 Firstly, since the latent variables are not marginalised, the parameters
 of the dynamical prior cannot be optimized without the risk of overfitting.
 %Because the latent variables aren't marginalized the parameters of the
%dynamical prior cannot be optimized without risk of overfitting. 
Further, the dimensionality of the latent space cannot be
determined: adding further dimensions always increases the likelihood
of the data. In this paper we build on recent developments in
variational approximations for Gaussian processes
\cite{Titsias09,BayesianGPLVM} to introduce a variational
Gaussian process dynamical system (VGPDS) where latent variables are
approximately marginalized through optimization of a rigorous lower
bound on the marginal likelihood. 
 As well as rigorously dealing with
uncertainty in the latent space, this allows the parameters of the
dynamical system to be determined and the dimensionality of the latent
space \textcolor{red}{[????]}
. The \textcolor{red}{this??} approximation enables the application of our model to time
series containing millions of dimensions and thousands of time
points. We illustrate this by modeling human motion capture data
and high dimensional video sequences.


% Developing a non linear probabilistic model for high dimensional time series data is a key challenge for machine learning. For data with strong correlations between the time series such a model can be composed of two components: a mapping from a state space to the observed data, and a temporal prior for  
% Statistical methods constitute a common approach for modelling dynamical systems that are too complex to be described by well-defined mathematical equations. In particular, Gaussian processes \cite{rasmussen-williams} offer a flexible, nonparametric backbone for many of these methodologies (e.g. \cite{Grimes06dynamicimitation, Girard03gaussianprocess}).
% For example, Girard \textit{et al.} \cite{Girard03gaussianprocess} train Gaussian processes on past system outputs and iteratively predict future ones by also propagating the uncertainty [added] in each intermediate step. 
% %However, this additional information is only expected to be useful if it comes from a large number of training data.
% Ko and Fox \cite{GP-Based} develop a Gaussian process framework able to integrate different types of Bayes filters, with the possibility of also incorporating secondary parametric models. 

% \par A more specific modelling direction in the same nonparametric context, is to assume that the observed data inherently lie in a low-dimensional manifold which is, though, unobserved. 
% \textcolor{blue}{This is a reasonable assumption, firstly because this is indeed the case for many real-life modelling scenarios (e.g. video tracking) and, secondly, because when the available dataset is small then the data cannot provide enough evidence for the presence of the whole set of dimensions.} 
% From this point of view, the actual modelling is done via dimensionality reduction. Again, Gaussian processes can be employed to perform this task nonlinearly, as algorithms based on linear methods, such as PPCA \cite{ppca} (or its Bayesian extension \cite{BayesianPCA}) and factor analysis \cite{FactorAnalysis}, fail at efficiently capturing the complex correlations between datapoints. Many such dynamical models for dimensionality reduction are motivated by the Gaussian process latent variable model (GPLVM) \cite{GPLVM, GPLVM2} which can be seen as a nonlinear generalisation of PCA. For example, Lawrence and Qui\~{n}onero Candela \cite{GPLVMBackConstraints} extend GPLVM by imposing locality constraints on the latent space. On the other hand, in \cite{hgplvm} GPLVM is extended with an additional temporal model which employs a GP prior that is able to generate smooth paths in the latent space.
% A similar way of thinking is followed in \cite{GPDM}, but the prior used encapsulates the Markov property, resulting in an auto-regressive model. These approaches typically rely on a training procedure which seeks for a MAP solution for the latent variables. 


% \par Optimising a MAP model makes the training procedure prone to overfitting and prevents the learning of the dynamical model's parameters, especially given that real-world datasets can often be small in sized and/or high-dimensional.
% % Existing state of the art methods find it difficult to cope with these challenges while at the same time being robust to overfitting.
% In this paper we propose a Bayesian framework for training Gaussian process dynamical systems. In our formulation, unlike MAP models, the latent variables are variationally marginalised out and the training procedure resists overfitting; furthermore, this marginalisation allows the model to automatically control its complexity by finding an effective number of dimensions for the latent space. 
% %
% Inspired by \cite{BayesianGPLVM}, we achieve this analytically intractable task by introducing variational pseudo-inputs for the system which are treated as variational parameters.
% %
%  However, the method in \cite{BayesianGPLVM} assumes independence across datapoints and it is unsuitable to be used with a dynamical model where the datapoint correlations also induce coupling in the variational distribution. The approach taken here is to employ a variational distribution which does not factorise w.r.t the datapoints and additionally uses fixed-point equations (as in \cite{OpperFixedPointCovariance}) to deal with the large number of variational parameters resulting from removing this restriction. As for predictions, we derive equations that find the test latent points $X_*$ by taking into account correlations with the training points $X$.
%
% The paper is laid out as follows: firstly we describe in detail the structure of the model and the approximate Bayesian procedure used for training. Then we demonstrate our method on motion capture data and we explain how this approach can naturally handle data with a very large number of dimensions, such as a raw video sequence. 



\section{The Model} 

Assume a multivariate times series dataset $\{\bfy_n,t_n\}_{n=1}^N$
where $\bfy_n \in \mathbb{R}^D$ is a data vector observed at time 
$t_n \in \mathbbm{R}_+$. We are especially interested in cases where each 
$\bfy_n$ is a high dimensional vector 
%($D$ can be equal to hundreds of thousands) 
and therefore we assume that there exists a
low dimensional manifold that governs the generation of the data. 
Specifically, we assume a {\em temporal} latent
function $\bfx(t) \in \mathbb{R}^Q$ (with $Q \ll D$), 
that comprises an intermediate {\em hidden} layer when generating the
data, so that $\bfy_n$ is produced from $\bfx_n = \bfx(t_n)$
according to      
\begin{equation}
\label{generative}
\mathit{y_{nd}} = f_d(\mathbf{x}_n) + \epsilon_{nd} \;, \;\;\; \epsilon_{nd} \sim \mathcal{N}(0, \beta^{-1}),
\end{equation}
where $\bff(\bfx) = [f_1(\mathbf{x}), \ldots, f_D(\mathbf{x})]^\T$
is a latent mapping from the low dimensional space to the observation space
and $\beta$ is the inverse variance of the white Gaussian noise. 
We do not want to make strong assumptions about the 
functional form of the latent functions $(\bfx, \bff)$\footnote{To
simplify our notation, we often write $\bfx$ instead of $\bfx(t)$
and $\bff$ instead of $\bff(\bfx)$. Later we also use a similar 
convention for the kernel functions by often writing them as $\mathit{k}_f$ and
$\mathit{k}_x$.}. Instead 
we would like to infer them in a fully Bayesian non-parametric
fashion using Gaussian processes \cite{rasmussen-williams}.
Therefore, we assume that $\bfx$ is a multivariate Gaussian process 
indexed by time $t$ and $\bff$ is a different multivariate 
Gaussian process indexed by $\bfx$, and we write 
\begin{eqnarray}
x_q(t)  & \sim & \mathcal{GP}(0, k_x(t_i,t_j)), \ \ q=1,\ldots,Q, \\     
f_d(\bfx)  & \sim & \mathcal{GP}(0, k_f(\bfx_i,\bfx_j)), \ \ d=1,\ldots,D.
\label{eq:GPpriors}
\end{eqnarray}
Here, the individual components of the latent function $\bfx$ 
have \textcolor{red}{are???} taken to be independent sample paths drawn from a Gaussian process with kernel or covariance function 
\textcolor{red}{["kernel or covariance function"???]}
$k_x(t_i,t_j)$. Similarly, the components of $\bff$ are independent 
draws from a Gaussian process with covariance function $k_f(\bfx_i,\bfx_j)$. 
These covariance functions, parametrized by parameters 
$\boldsymbol \theta_x$ and $\boldsymbol \theta_f$ respectively,  play 
very distinct roles in the model. More precisely, 
$k_x$ determines the properties of each temporal 
latent function $x_q(t)$. For instance, the use of an
OU covariance function yields a Gauss-Markov autoregressive 
process for $x_q(t)$, while the squared-exponential 
kernel gives rise to very smooth and 
non-Markovian processes. The choice of a particular $k_x$ 
is not expected to play a strictly determinative role for 
the overall performance of the method because the kernel's 
hyperparameters are optimised to fit the data in any case. However, 
certain kernels can fit better the characteristics of a given dataset. 
In our experiments, we focus on the squared exponential covariance function
(RBF), the Matern $3/2$ 
which is only once
differentiable and a periodic covariance function
\cite{rasmussen-williams, MacKay98} which
can be used when data exhibit strong periodicity. These kernel
functions take the form:
\begin{eqnarray}
k_{x(rbf)} \left( \mathit{t_i, t_j} \right) 
& = & \sigma_{rbf}^2 e^{- \frac{\left( t_i - t_j \right)^2}{\left(
      2l_t^2 \right)}}, 
\ \ k_{x(mat)} \left( t_i, t_j \right) =  
\sigma_{mat}^2 \left( 1 + \frac{\sqrt{3} |t_i - t_j|}{l_t} \right)
		e^{\frac{ - \sqrt{3} |t_i - t_j|}{l_t} }, \nonumber \\
%\label{matern} 
k_{x(per)} \left( \mathit{t_i, t_j} \right) 
& = & 
	\sigma_{per}^2 e^{-\frac{1}{2} \frac{sin^2 \left( \frac{2
                \pi}{T} \left( t_i - t_j \right) \right) }{l_t} }. 
 \label{eq:temporalkernels}
\end{eqnarray}
%  
%
%\begin{align}
%k_{t(rbf)} \left( \mathit{t_i, t_j} \right) = {} &
%		\sigma_{rbf}^2 exp \left( 
%			- \left( \mathit{t_i - t_j} \right)^2)/ \left( 2l_t^2 \right)
%			\right) \label{rbf}  \\
%k_{t(mat)} \left( \mathit{t_i, t_j} \right) = {} & 
%		\sigma_{mat}^2 \left( 1 + \sqrt{3} |t_i - t_j|/l_t \right)
%		exp \left( - \sqrt{3} |t_i - t_j|/l_t \right)  \label{matern} \\
%k_{t(per)} \left( \mathit{t_i, t_j} \right) = {} & 
%	\sigma_{per}^2 exp \left(
%	  -\frac{1}{2} \frac{sin^2 \left( \frac{2 \pi}{T} \left( t_i - t_j \right) \right) }{l_t}
%	    \right)  \label{rbfperiodic}
%\end{align}
%
%
%\begin{comment}
%\textcolor{blue}{Given the above, the $N \times N$ covariance matrix $K_t$, built using any of the aforementioned covariance functions, is able to capture the temporal continuity in the dataspace by taking into account the times at which each point is being observed.} The choice of a particular kernel for the temporal GP prior is not expected to play a strictly determinative role for the overall performance of the method because the kernel's hyperparameters are optimised to fit the data in any case. However, different kernel functions have different properties, especially when more sophisticated ones are being considered, for example combinations of the above. Therefore, in practice, choosing a kernel that agrees with the nature of the data can make the optimisation process faster and more efficient. 
%\end{comment}
% 
On the other hand, the covariance function $k_f$ determines the
properties of the latent mapping $\bff$ that maps each  low 
dimensional variable $\bfx_n$ to the observed vector $\bfy_n$. We wish
this mapping to be a non-linear but smooth function, and thus a suitable choice
is the squared exponential kernel
\begin{align}
\mathit{k_f} \left( \mathbf{x}_i, \mathbf{x}_j \right) = {} &  
		\sigma_{ard}^2 e^{
			- \frac{1}{2} \sum_{q=1}^{Q}  w_q \left(
                          \mathit{x_{i,q} - x_{j,q}} \right) ^2 },
\label{rbfard}
\end{align}
which assumes a different scale $w_q$ for each latent dimension. This,   
similarly to the variational Bayesian formulation of the GP-LVM 
\cite{BayesianGPLVM}, can enable an automatic relevance
determination procedure, i.e.\ allowing Bayesian training to ``switch off''
unnecessary dimensions by driving the values of 
the corresponding scales to zero.

To summarize the above Bayesian non-parametric model, 
we can write down its finite joint probability density 
function induced by the finite observations. We introduce first some 
useful notation. The matrix 
$\mathit{Y} \in \mathbb{R}^{N \times D}$ will collectively  denote 
all observed data so that its $n$th row corresponds to the data 
point $\bfy_n$. Similarly, the matrix  
$F \in \mathbb{R}^{N \times D}$ will denote the mapping latent variables,
i.e.\ $f_{nd} =f_d(\bfx_n)$, associated with observations $Y$ from
(\ref{generative}). Analogously,  $X \in \mathbb{R}^{N \times
  Q}$ will store all low dimensional latent variables 
$x_{nq}=x_q(t_n)$. Further, we will refer to columns of these 
matrices by the vectors $\bfy_d,\bff_d, \bfx_q \in
\mathbbm{R}^N$. The joint probability density is written as
\begin{equation}
\label{joint}
p(Y,F, X | \bft) = p(Y|F) p(F|X) p(X |\bft)= 
 \prod_{d=1}^D  p(\mathbf{y}_d | \mathbf{f}_d) p(\mathbf{f}_d |
 \mathit{X}) \prod_{q=1}^Q p(\mathbf{x}_q | \mathbf{t}),
\end{equation}
where $\bft \in \mathbb{R}^N$ and $p(\mathbf{y}_d | \mathbf{f}_d)$ 
is a Gaussian likelihood function term defined from  (\ref{generative}). 
Further, $p(\mathbf{f}_d | \mathit{X})$ is a marginal GP prior 
such that 
\begin{equation}
\label{priorF}
p(\bff_d | \mathit{X}) = \mathcal{N}(\bff_d |\mathbf{0}, \mathit{K_{NN}}),
\end{equation}
where $\mathit{K}_{NN}= \mathit{k}_f(X,X)$ is the covariance matrix
defined by the kernel function $\mathit{k}_f$ and similarly 
$p(\bfx_q|\mathbf{t})$ is the marginal GP prior associated with 
the temporal function $x_q(t)$,  
\begin{equation}
p(\bfx_q|\mathbf{t}) = \mathcal{N} \left( \mathbf{x}_q | \mathbf{0},
  \mathit{K_t} \right),
\label{priorXgivenT}
\end{equation}
where $K_t = k_x(\bft,\bft)$ is the covariance matrix obtained by
evaluating the kernel function $\mathit{k}_x$ on the observed times
$\bft$. 
 
Bayesian inference using the above model poses a huge 
computational challenge as, for instance, marginalization 
of the variables $X$ that appear non-linearly inside the kernel 
matrix $K_{NN}$ is troublesome. Practical approaches that have been considered until now
(e.g. \cite{hgplvm, GPDM}) marginalise out only $F$ and seek a MAP solution 
for $X$.
%
% Up until now, the only practical approach  that has been
%considered (e.g. \cite{hgplvm, GPDM}) is to marginalises out only $F$ and seek a MAP solution 
%for $X$.
 In the next section we describe how efficient variational 
approximations can be applied to marginalize $X$ by extending the 
framework of \cite{BayesianGPLVM}.

\subsection{Variational Bayesian training} 

The key difficulty with the Bayesian approach is propagating the prior
density $p(X|\bft)$ through the nonlinear mapping. This
mapping gives the expressive power to the model, but simultaneously
renders the associated marginal
likelihood,
\begin{equation}
\label{marginalLikelihood}
p(Y | \bft) =  \int p( Y | F) p(F | X ) p(X | \bft) \, \mathrm{d} X \,\mathrm{d}F,
\end{equation}
%This quantity, along with the posterior distribution $p\left( X | Y\right)$, is essential in optimising the model and performing model selection or prediction tasks, as will be explained in later sections. 
intractable. We now invoke the variational Bayesian methodology
to approximate the integral. Following a standard procedure \cite{bishop}, we introduce a variational distribution
$q(\Theta)$ and compute the Jensen's lower bound $\mathcal{F}_v$ on the logarithm of \eqref{marginalLikelihood},
%
\begin{align}
\mathcal{F}_v(q, \boldsymbol \theta) = {}& \int q(\mathit{\Theta}) \log 
		\frac{ p(Y | F) p(F | X ) p(\mathit{X} | \mathbf{t})}
			 {q(\mathit{\Theta})}  \, \mathrm{d} X \,\mathrm{d}F,
% 	    \nonumber \\
% 	      = {}& \sum_{d=1}^D \int q(\Theta) \log \left( p(\bfy_d | \bff_d) p(\bff_d | X) \right) dX d \bff_d -
% 		    \int q(\Theta) \frac{p(X|\bft)}{q(\Theta)} dX
		 \label{jensens1}
\end{align}
%
%
 %
where $\boldsymbol \theta$ denotes the model's parameters. 
However, the above form of the lower bound is problematic becauce 
$X$ (in the GP term $p(F|X)$) appears non-linearly inside the kernel
matrix $K_{NN}$ making the integration over $X$ difficult. As shown  
in \cite{BayesianGPLVM}, this intractability is removed by 
applying the ``data augmentation'' principle. 
More precisely, we  augment the joint probability
model in (\ref{joint}) 
by including $M$ extra samples of the GP latent mapping $\bff$, 
known as inducing points, so that  $\bfu_m \in \mathbb{R}^D$
is such a sample. The inducing points are evaluated at a set of
pseudo-inputs $\tilde{X} \in \mathbb{R}^{M \times Q}$. %  and calculating
% their function values $U \in \mathbb{R}^{M \times D}$.  This trick is
% inspired by sparse GP regression approaches (e.g. \cite{Titsias09,
%   Lawrence02, Quin-Rasm05}).
The augmented joint probability density takes the form
\begin{equation}
 \label{augmentedJoint}
p(Y,F, U,X,\tilde{X} | \bft) = \prod_{d=1}^D p(\mathbf{y}_d | \mathbf{f}_d) p(\mathbf{f}_d | \mathbf{u}_d, \mathit{X})
p(\bfu_d | \tilde{X})  p(X | \mathbf{t}),
\end{equation}
where $p(\bfu_d | \tilde{X})$ is a zero-mean Gaussian with a
covariance matrix $K_{MM}$ constructed using the same function as for
the GP prior \eqref{priorF}. By dropping $\tilde{X}$ from our
expressions, we write the augmented GP prior analytically (see
\cite{rasmussen-williams}) as
\begin{equation}
 \label{priorF2}
p(\bff_d | \bfu_d, X) =  \mathcal{N}  \left( \bff_d | K_{NM} K_{MM}^{-1} \bfu_d , K_{NN} - K_{NM} K_{MM}^{-1} K_{MN} \right).
\end{equation}
%A key result in \cite{BayesianGPLVM} is that the following 
% variational density, $q(\Theta)$, yields a tractable lower bound 
%(computed analogously to (\ref{jensens1}))
A key result in \cite{BayesianGPLVM}
 is that a tractable lower bound 
(computed analogously to (\ref{jensens1})) can be obtained by employing the following variational density:
\begin{equation}
\label{varDistr}
q(\mathit{\Theta}) = q(F, U,X) = q(F | U, X) q(U) q(X) = \prod_{d=1}^D p(\bff_d | \bfu_d, X )q(\bfu_d) q(X),
%q(\mathit{X}, U, F) = p(F | X, U) q(U) q(X)
\end{equation}
%
where $q(X) = \prod_{q=1}^Q \mathcal{N} \left( \bfx_q | \bfmu_q, S_q
\right)$ and $q(\bfu_m)$ is an arbitrary variational distribution. 
In  \cite{BayesianGPLVM} the next step is to assume
independence in $q(X)$ (i.e.\ diagonal $S_q$). 
\textcolor{red}{[also there we have $S_n$ which is $Q \times Q$, here it is $N \times N$ (it's equivalent though)]}
Here, in contrast, 
the posterior over the latent variables will have strong correlations, 
so $S_q$ is taken to be a $N \times N$ full covariance
matrix. Optimization of the variational lower bound provides 
an approximation to the true posterior $p(X|Y)$ \textcolor{red}{by $q(X)$}.
%
In the augmented probability model, the ``difficult`` term $p(F | X)$ appearing in \eqref{jensens1} is now replaced with \eqref{priorF2} and, eventually, it cancels out with the first factor of the variational distribution \eqref{varDistr} so that $F$ can now be marginalised out analytically.
% 
%
%Factorising the variational distribution as shown in \eqref{varDistr} is also useful for recovering an approximation $q(X)$ to the true posterior $p(X|Y)$.
 Given the above and after breaking the logarithm in \eqref{jensens1}, we obtain the final form of the lower bound (see supplementary material for more details)
%
\begin{equation}
\label{jensens}
\mathcal{F}_v(q, \boldsymbol \theta) = 
\hat{\mathcal{F}}_v - \text{KL}(q(X) \parallel p(X|\bft)),
\end{equation}
%
with $\hat{\mathcal{F}}_v =\int q(X) \log p( Y | F ) p( F | X) \,
\mathrm{d} X \,\mathrm{d}F$. Both terms in \eqref{jensens} are now
tractable. Note that the first of the above terms involves the data while
the second one only involves the prior. All the information regarding
datapoint correlations is captured in the $\text{KL}$ term and the
connection with the observations comes through the variational
distribution. Therefore, the first term in \eqref{jensens} has the
same analytical solution as the one derived in \cite{BayesianGPLVM}.
%
% This solution is also very similar to the one found in \cite{Titsias09} for sparse GP regression with the main difference being that here we have expectations on the covariance matrices to average w.r.t the variational distribution. 
%For more details concerning the derivation of this bound and the computation of the $\Psi$ quantities see \cite{BayesianGPLVM} and \cite{Titsias09}. 
\eqref{jensens} can be maximized by using gradient-based
methods\footnote{See supplementary material for more detailed
  derivation of \eqref{jensens} and for the equations for the
  gradients.}. However, when not factorizing $q(X)$ across data points
yields $O(N^2)$ variational parameters to optimize. 
This issue is addressed in the next section.



\subsection{Reparametrization and optimisation \label{optimisation}} 
%Following \cite{OpperFixedPointCovariance}, we can re-express the variational parameters using $K_t$

The optimization involves the model parameters 
$\boldsymbol \theta = (\beta, \boldsymbol \theta_f, \boldsymbol \theta_t)$, 
the variational parameters $\{ \bfmu_q, S_q \}_{q=1}^Q$ from $q(X)$ and the inducing points\footnote{We will use the term ``variational parameters'' to refer only to the parameters of $q(X)$ although the inducing points are also variational parameters.} $\tilde{X}$.
%Since $\boldsymbol \theta_f$ and $\boldsymbol \theta_t$ exist in different terms (see \eqref{jensens}), the only coupling between the prior on the latent space (which induces the datapoint correlations) and the likelihood term (which involves observations) is made via the variational parameters. 

% \par In our formulation, unlike \cite{BayesianGPLVM}, the $Q$ variational covariances $S_q$ are full $N \times N$ matrices. 
% Therefore, 
Optimization of the variational parameters appears challenging, due to
their large number and due to the correlations between them. However, by
reparametrizing our $O \left( N^2 \right)$ variational parameters
according to the framework described in
\cite{OpperFixedPointCovariance} we can obtain a set of $O(N)$ 
less correlated variational parameters. Specifically, we first take the derivatives of the variational bound \eqref{jensens} w.r.t $S_q$ and $\bfmu_q$ and set them to zero, in order to find the stationary points,
%, one can estimate the variational parameters using fixed-point equations instead of directly optimising them. To obtain such equations we set the variational bound's derivative w.r.t $S_q$ to zero and solve for $S_q$:
\begin{equation}
S_q = \left( \mathit{K}_t^{-1} + \Lambda_q \right)^{-1} \;\;\; \mbox{ \textit{and} } \;\;\;  \boldsymbol \mu_q = K_t \bar{\boldsymbol \mu}_q, \label{SFixedPointQ}
\end{equation}
where $\Lambda_q = - 2\frac{\vartheta \mathit{\hat{F}_v(q, \boldsymbol
    \theta)}}{\vartheta \mathit{S_q}}$ is a $N \times N$ diagonal,
positive matrix and $\bar{\boldsymbol \mu}_q = \frac{\vartheta
  \hat{F}_v}{\vartheta \boldsymbol \mu_q}$ is a $N-$dimensional
vector.
%It is now obvious that the variational parameters are correlated,
%since they can be re-expressed as a function of $K_t$. 
The above stationary conditions tell us that, since 
$S_q$ depends on a diagonal matrix $\Lambda_q$, 
we can reparametrize it using only the $N-$dimensional diagonal of that matrix, denoted by $\boldsymbol \lambda_q$.
Then, we can optimise the $2 (Q \times N)$ parameters $( \boldsymbol \lambda_q$, $\bar{\bfmu}_q)$ and obtain the original parameters using \eqref{SFixedPointQ}.
%One important fact which must be emphasized is that, with the aforementioned methodology, each $\boldsymbol \mu_q$ as well as $S_q$ is now re-parametrised using $K_t$ which means that they now include kernel hyperparameters $\boldsymbol \theta_t$; therefore, the quantity $\frac{\partial \mathbf{F}}{\partial \theta_t}$ should be amended with the partial derivatives as appropriate.



\subsection{Learning from multiple sequences \label{sequences}}

Our objective is to model multivariate time series. A given data set may consist of a group of independently observed sequences, each with a different length (e.g. in human motion capture data several walks from a subject). We would like our model to capture the underlying commonality of these data. 
%The proposed approach can naturally handle such cases by learning a shared latent space (and shared model parameters) for all sequences while ignoring correlations between them. 
We handle this by associating each sequence $s$ with a different
subspace $X^{(s)}$ of a shared latent manifold; \textcolor{red}{the
mappings $\bff$ and the model parameters are shared for all sequences [KEEP THIS??]}. 
 More specifically, if $Y = \left[ Y^{(1)}, ...,
  Y^{(S)} \right]^\T $ is a concatenation of $S$ blocks (sequences),
then each one is assumed to be generated by $Y^{s} = \bff (X^{(s)})$
plus some noise and then the likelihood
is written in the form,
\begin{equation}
 p \left( Y^{(1)}, ..., Y^{(S)} | \bff  \right) p \left( \bff | X^{(1)}, ..., X^{(S)} \right)
\end{equation}
%where $\bff = \left[ \bff^{(1)}, ..., \bff^{(S)} \right]^\T$. 
To account only for time correlations within the same sequence we
force the time covariance matrix $K_t$ to be block-diagonal with each
block capturing the dependencies implied by the datapoints of a single
sequence.


\section{Predictions} 

Our algorithm models the temporal evolution of a dynamical system. It
should be capable of generating completely new sequences or
reconstructing missing observations from partially observed data. For
generating novel sequence given training data the model requires a
time vector $\bft_*$ as input and computes a density $p(Y_* | Y, \bft,
\bft_*)$. For reconstruction of partially observed data the timestamp
information is additionally accompanied by a partially
observed sequence $Y_*^{p} \in \mathbb{R}^{N_* \times D_p}$ from the
whole $Y_* = (Y_*^{p}, Y_*^{m})$, where $p$ and $m$ are set of indices
indicating the present (i.e. observed) and missing dimensions of $Y_*$
respectively, so that $p \cup m= \{1,\ldots,D\}$.  We reconstruct the
missing dimensions by computing the Bayesian predictive distribution
$p(Y_*^{m}| Y_*^{p}, Y, \bft_*, \bft)$. The predictive densities can
also be used as estimators for tasks like generative Bayesian
classification. For optimal performance test data is presented jointly
to model the correlations in the augmented set $[Y, Y_*]^\T$
correctly. Whilst time stamp information is always provided, in the
next section we drop its dependence to avoid notational clutter.


\subsection{Totally unobserved data \label{unobservedData}}
%Firstly, we discuss how the model can predict or generate a set of outputs $Y_*$ given only an input time-vector $\bft_*$. 
To approximate the predictive density, we will need to introduce the underlying latent function values $F_* \in \mathbb{R}^{N_* \times D}$ (the noisy-free version of $Y_*$) and the latent variables $X_* \in \mathbb{R}^{N_* \times Q}$. We  write the predictive density as
\begin{eqnarray}
p(Y_* | Y) & = & \int p(Y_*, F_*, X_*| Y_*, Y) d F_* d X_* =  \int p(Y_* | F_*)  p(F_*|X_*, Y) p(X_*|  Y) d F_* d X_* .
\label{eq:predictive1}
\end{eqnarray}
The term $p(F_* |X_*, Y)$ is approximated according by
\begin{eqnarray}
q(F_*|X_*) & = & \int \prod_{d \in D} p(\bff_{*,d} | \bfu_d, X_*)  q(\bfu_d) d \bfu_d 
	    = \prod_{d \in D} q(\bff_{*,d} | X_*)  ,
\end{eqnarray}
where $q(\bff_{*,d} | X_*)$ is a Gaussian that can be computed analytically.% , since $q(\bfu_d)$ is also a Gaussian, found after doing some calculations in the $\tilde{\mathcal{F}}_v$ term of \eqref{jensens}.
% $$
% q(\bff_{*,d}^m | X_*) = \mathcal{N}(\bff_{*,d}^m| \beta K_{N_* M} 
% (K_{MM} + \beta \Psi_2)^{-1} \Psi_1^{T} \bfy_d, K_{N_* N_*} - 
%  K_{N_* M} \left[ K_{M M}^{-1}  - (K_{M M} + \beta \Psi_2)^{-1} \right] 
%  K_{N_* M}^{T})
% $$
The term $p(X_*| Y)$ in eq. (\ref{eq:predictive1}) is approximated by
a Gaussian variational distribution $q(X_*)$,
%
\begin{align}
p(X_* | Y) \approx {}& \int  p(X_* | X) q(X) dX = \la  p(X_* | X) \ra_{q(X)} = q(X_*) = \prod_{q=1}^Q q(\bfx_{*,q}),\label{qxstar}
\end{align}
%
where $p( X_{*,q} | X)$ can be found from the conditional GP prior
(see \cite{rasmussen-williams}). We can then write
%
\begin{equation}
\label{qxstar2}
\bfx_{*,q} = \boldsymbol \alpha \bfx_q + \boldsymbol \epsilon,
\end{equation} 
%
where $\boldsymbol \alpha = K_{*N}K_t^{-1}$ and 
$\boldsymbol \epsilon \sim \mathcal{N} \left( \bfz, K_{**} - K_{*N K_t^{-1} K_{N*}}\right)$. Also, $K_t = k_t(\bft, \bft)$, $K_{*N} = k_t(\bft_*, \bft)$ and $K_{**} = k_t(\bft_* \bft_*)$. 
Given the above, we know a priori that \eqref{qxstar} is a Gaussian and by taking expectations over $q(X)$ in the r.h.s. of \eqref{qxstar2} we find the mean and covariance of $q(X_*)$. Substituting for the equivalent forms of $\bfmu_q$ and $S_q$ from section \ref{optimisation} we obtain the final solution
%
\begin{align}
 \mu_{x_{*,q}} = {}& \bfk_{*N} \bar{\mu}_q \\
  \text{var}(x_{*,q}) = {}& k_{**} - \bfk_{*N} (K_t + \Lambda_q^{-1})^{-1} \bfk_{N*}.
\end{align}
%
\eqref{eq:predictive1} can then be written as:
%`
\begin{align} 
p(Y_*| Y) {}& =  \int p(Y_*| F_*)  q(F_*|X_*) q(X_*) d F_* d X_* = \int p(Y_* | F_*) \la q(F_* | X_*) \ra_{q(X_*)} d F_* \label{eq:predictive2}
\end{align}
%
Although the expectation appearing in the above integral is not a Gaussian, its moments can be found analytically \cite{rasmussen-williams, Girard03gaussianprocess},
%
\begin{align}
 \mathbb{E}(F_*) ={}&  B^\T \Psi_1^* \label{meanFstar} \\
 \text{Cov}(F_*) ={}& B^\T \left( \Psi_2^* - \Psi_1^* (\Psi_1^*)\T \right) B + \Psi_0^* I - \text{Tr} \left[ \left( K_{MM}^{-1} - \left( K_{MM} + \beta \Psi_2 \right)^{-1} \right) \Psi_2^* \right] I,
\end{align}
%
where $B = \beta \left( K_{MM} + \beta \Psi_2 \right)^{-1} \Psi_1^\T
Y$, $\Psi_0^* = \la k_f(X_*, X_*) \ra$, $\Psi_1^* = \la K_{M*} \ra$
and $\Psi_2^* = \la K_{M*} K_{*M} \ra$. All expectations are taken
w.r.t. $q(X_*)$ and can be calculated analytically, while $K_{M*}$
denotes the cross-covariance matrix between the training inducing
inputs $Z$ and $X_*$. Finally, since $Y_*$ is just a noisy version of
$F_*$, the mean and covariance of \eqref{eq:predictive2} is just
computed as: $\mathbb{E}(Y_*) = \mathbb{E}(F_*)$ and $\text{Cov}(Y_*)
= \text{Cov}(F_*) + \beta^{-1} I_{N_*}$.


\subsection{Partially observed test data}

The expression for the predictive density $p(Y_*^m | Y_*^p, Y)$ follows exactly as in section \ref{unobservedData} but we need to compute probabilities for $Y_*^m$ instead of $Y_*$ and $Y$ is replaced with $(Y, Y_*^p)$ in all conditioning sets. Similarly, $F$ is replaced with $F^m$. Now $q(X_*)$ cannot be found analytically as in eq. \ref{unobservedData}; instead, it is optimised so that $Y_*^p$ are taken into account. This is done by maximising the variational lower bound on the marginal likelihood:
\begin{align}
p(Y_*^p, Y) ={}&  \int p(Y_*^p, Y|X_*, X) p(X_*, X) \text{d} X_* \text{d} X \nonumber \\
			={}&  \int p(Y^m | X) p(Y_*^p, Y^p|X_*, X) p(X_*, X) \text{d} X_* \text{d} X,  \nonumber
\end{align}  
Assuming a variational distribution 
$q(X_*, X)$ and using Jensen's inequality we obtain the 
lower bound 
\begin{eqnarray}
& & \int q(X_*, X) \log \frac{ p(Y^m | X) 
p(Y_*^p, Y^p|X_*, X) p(X_*,X)}{ q(X_*, X)} \text{d} X_* \text{d} X \nonumber \\ 
& = & \int q(X) \log p(Y^m | X) \text{d} X 
+  \int q(X_*,X) \log p(Y_*^p, Y^p|X_*, X) \text{d} X_* \text{d} X  \nonumber \\
& - & \text{KL}[q(X_*,X) || p(X_*, X)] \label{partialPredLowerBound}
\end{eqnarray}  
%
This quantity can now be maximized in the same manner as for the bound
of the training phase. Unfortunately, this means that the variational
parameters that are already optimised from the training procedure
cannot be used here because $X$ and $X_*$ are coupled in $q(X_*,X)$. A
much faster but less accurate method would be to decouple the test
from the training latent variables by imposing the factorisation
$q(X_*, X) = q(X) q(X_*)$. Then, equation
\eqref{partialPredLowerBound} would break into terms containing $X$,
$X_*$ or both. The ones containing only $X$ could then be treated as
constants.


\section{Handling Very High Dimensional Datasets}

Given that our training framework is built on a sparse approach, the
approximation is tractable for relatively large datasets (thousands of
time points, $N$). However \textcolor{red}{["however"? Isn't it like contrasting a positive fact with another positive fact??]},
 the model scales only
linearly with the number of dimensions $D$. Specifically, the number
of dimensions only matters when performing calculations involving the
data matrix $Y$. In the final form of the lower bound (and
consequently in all of the derived quantities, such as gradients) this
matrix only appears in the form $Y Y^\T$ which can be precomputed. This
means that, when $N \ll D$, we can calculate $Y Y^\T$ only once and
then substitute $Y$ with the SVD (or Cholesky decomposition) of $Y
Y^\T$. In this way, we can work with an $N \times N$ instead of an $N
\times D$ matrix. Practically speaking, this allows us to work with data sets involving millions of features. In our experiments we model directly the pixels of HD quality video, exploiting this trick.


\section{Experiments}

We consider two different types of high dimensional time series \textcolor{red}{[the mocap is 59-dim, is that considered to be high dim?]}, a human motion capture data set consisting of different walks and high resolution video sequences. The experiments are intended to explore the various properties of the model and to evaluate its performance in different tasks (prediction, reconstruction, generation of data). More figures and examples are included in the supplementary material.
% For this reason, we first investigated how the model can learn from multiple training sequences and predict new, independent ones given some partial information. Secondly, we studied \highlight{[...]}

\subsection{Motion capture data}

We followed \cite{Taylor,gplvmLarger} in considering motion capture
data of walks and runs taken from subject 35 in the CMU motion capture
database. We treated each motion as an independent sequence.  The data
set was constructed and preprocessed as described in
\cite{gplvmLarger}. This results in 2613 59-dimensional frames split
into 31 training sequences with an average length of 84 frames each.

The model is jointly trained, as explained in section \ref{sequences},
on both walks and runs, challenging the algorithm to learn a common latent
space for these motions. At test time we investigate the ability of
the model to reconstruct test data from a previously unseen sequence
given partial information for the test targets. This is tested once by
providing only the dimensions which correspond to the body of the
subject and once by providing those that correspond to the legs.
%
%only the dimensions that correspond to the body of the subject and once by providing those that correspond to the legs. The whole sequence is then reconstructed, as explained in section \ref{unobservedData}.
%

We compare with results in \cite{gplvmLarger}, which used MAP
approximations for the dynamical models, and against nearest
neighbour. We can also indirectly compare with the binary latent
variable model (BLV) of \cite{Taylor} which used a slightly different
data preprocessing. We assess the performance using the cumulative
error per joint in the scaled space defined in \cite{Taylor} and by
the root mean square error in the angle space suggested by
\cite{gplvmLarger}. Our model was initialized with nine latent
dimensions. We performed two runs, once using the Matern covariance function for the dynamical prior
and once using the RBF. From table
\ref{motionCaptureTable} we see that the variational Gaussian process
dynamical system considerably outperforms the other approaches. 
%
The appropriate latent space dimensionality for the data was automatically
inferred by our models, since the one that used the RBF kernel retained four dimensions 
whereas the model using the Matern kept only three.
%
%The
%models automatically inferred that the
%appropriate latent space dimensionality for the data was three. 
%
The
other latent dimensions were completely switched off by the ARD
parameters. 
The best performance for the legs and the body reconstruction was achieved by the VGPDS model that used the Matern and the RBF covariance function respectively.
%The best performance for legs reconstruction was achieved by the model using the Matern covariance whereas the best performance for body reconstruction used the RBF.


% An additional interesting remark is that our model automatically learns a 3-dimensional latent space, completely switching off the rest of the dimensions. As can be seen in table \ref{motionCaptureTable}, GP-LVM also performed best with $Q=(3,4)$, but in that case, the number of dimensions is a hyperparameter which can only be found after exhaustive model selection. 
% \textcolor{blue}{A second interesting issue to mention, is that our method performs better in the reconstruction of legs with the Matern kernel, while for the smoother body movements the RBF kernel seems to be more appropriate for modelling the dynamics.}



\begin{table}[h]
\caption{
\small{
Errors obtained for the motion capture dataset considering nearest neighbour in the angle space (NN) and in the scaled space(NN sc.), GPLVM, BLV and VGPDS. CL / CB are the leg and body datasets as preprocessed in \cite{Taylor}, L and B the corresponding datasets from \cite{gplvmLarger}. SC corresponds to the error in the scaled space, as in Taylor et al. while RA is the error in the angle space. The best error per column is in bold. }}
\label{motionCaptureTable}
\begin{center}
\begin{tabular}{c||c|c|c|c|c|c}
%\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
Data & CL & CB & L & L & B & B \\  \hline
Error Type & SC & SC & SC & RA & SC & RA \\
\hline \hline
BLV 			       & 11.7 & \textbf{8.8} & - & - & - & - \\  \hline
NN sc.   		       & 22.2 & \textbf{20.5} & - & - & - & - \\ \hline
GPLVM (Q = 3)	       & - & - & 11.4 & 3.40 & 16.9 & 2.49 \\ \hline
GPLVM (Q = 4)	       & - & - & 9.7  & 3.38 & 20.7 & 2.72 \\ \hline
GPLVM (Q = 5)	       & - & - & 13.4 & 4.25 & 23.4 & 2.78 \\ \hline
NN sc.  		       & - & - & 13.5 & 4.44 & 20.8 & 2.62 \\ \hline
NN 		 		       & - & - & 14.0 & 4.11 & 30.9 & 3.20 \\ \hline
VGPDS (RBF)        & - & - & 8.19 & 3.57 & \textbf{10.73} & \textbf{1.90} \\ \hline
VGPDS (Matern 3/2) & - & - & \textbf{6.99} & \textbf{2.88} & 14.22 & 2.23 \\
\end{tabular}
\end{center}
\end{table}

%Lengthscales retained: rbf: 2 large 1 small. matern: 2 large 2 small

\subsection{High dimensional video sequences}

For our second set of experiments we considered video sequences. Such
sequences are typically preprocessed before modelling to extract
informative features and reduce the dimensionality of the
problem. Here we work directly with the raw pixel values to
demonstrate the ability of the VGPDS to model data with a vast number
of features. This also allows us to directly sample video from the
learned model.

\highlight{Need a bit more detail here (also mention that only ocean is HD)}
Firstly we used the model to reconstruct partially observed frames
from video test sequences. The mean squared error per pixel was
measured to compare with the $k-$Nearest Neighbour (NN) method, for $k
\in (1,..,5)$ (we only present the error achieved for the best $k$ per
case). The datasets considered are the following: firstly, the 'Missa'
dataset\footnote{www.cipr.rpi.edu}, a standard benchmark used in image
processing. This 103,680-dimensional video, showing a woman talking
for 150 frames, is quite challenging as there are translations in the
pixel space. We also considered a HD video of dimensionality $9 \times
10^5$ that shows an artificially created scene of ocean waves as well
as a $?-$dimensional video\footnote{available at: ...} showing a dog
running for $60$ frames. The later is a dataset periodic in nature but
only approximately, as it is a real-world video (otherwise the NN
method would achieve perfect reconstruction). For the first two videos
we used the Matern and RBF kernel respectively to model the dynamics and interpolated to
reconstruct blocks of frames chosen from the whole sequence. For the
 'dog' dataset we constructed a compound kernel $k_x = k_{x(rbf)} +
k_{x(periodic)}$, where the RBF term is employed to capture the
divergence from the approximately periodic pattern. We then used our
model to reconstruct the last 7 frames, thus performing
extrapolation. As can be seen in table \ref{videoResultsTable}, our
method outperformed NN in all cases. The results are also demonstrated
visually in figure \ref{fig:video1} and in the supplementary material.



% given a video sequence from which blocks of frames have been randomly removed to form a test set. During the test phase we present only half of each test frame, along with its corresponding time point and attempt to reconstruct it with our model. The mean squared error per pixel is compared to the $k$-Nearest Neigbour method, for $k \in (1,..,5)$ (we only present the error achieved for the best $k$ per case). For this experiment we used the 'Missa' dataset\footnote{www.cipr.rpi.edu}, a standard benchmark in image processing. This 103,680-dimensional video, showing a woman talking for 150 frames, is quite challenging as there are translations in the pixel space.
% We also evaluated the model's performance for the same task by using a HD video of dimensionality close to $10^6$ that shows an artificially created scene of ocean waves. For both videos we used the Matern kernel \eqref{matern} to capture the dynamics in the latent space and the error obtained was smaller than that of NN (see table \ref{videoResultsTable}). For demonstration we also present two snapshots of the original and the reconstructed frames in figure \ref{fig:missa}. 


\begin{table}[h]
\caption{
\small{
The mean squared error per pixel for VGPDS and NN for the three datasets (measured only in the missing inputs). The number of latent dimensions selected by our model is in parenthesis. 
} }
\label{videoResultsTable}
\begin{center}
\begin{tabular}{c||l|l|l}
%\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
  & Missa & Ocean & Dog \\
\hline \hline
VGPDS  & 3.12 ($Q =...$) & 9.36 ($Q = 9$)  & 4.01 ($Q = 6$) \\  \hline
NN  & 3.26 & 9.53 & 4.15 \\
\end{tabular}
\end{center}
\end{table}

\begin{figure}[ht]
\begin{center}
\subfigure[]{
	\includegraphics[width=0.23\textwidth]{images/missa/Missafr26Varmu}
	\label{fig:missa1}
}
\subfigure[]{
	\includegraphics[width=0.23\textwidth]{images/missa/Missafr26Original}
	\label{fig:missa2}
}
\subfigure[]{
	\includegraphics[width=0.23\textwidth]{images/missa/Missafr26NN}
	\label{fig:missa3}
}
\subfigure[]{
	\includegraphics[width=0.23\textwidth]{images/missa/Missa22All}
	\label{fig:missa4}
}
\subfigure[]{
	\includegraphics[width=0.23\textwidth]{images/ocean/Varmu27}
	\label{fig:ocean1}
}
\subfigure[]{
	\includegraphics[width=0.23\textwidth]{images/ocean/NNmu27}
	\label{fig:ocean2}
}
\subfigure[]{
	\includegraphics[width=0.23\textwidth]{images/dog/scalesInit}
	\label{fig:scalesDogInit}
}
\subfigure[]{
	\includegraphics[width=0.23\textwidth]{images/dog/scalesOpt}
	\label{fig:scalesDogOpt}
}
\end{center}
\caption{\small{
\ref{fig:missa1} and \ref{fig:missa3} demonstrate the reconstruction achieved by VGPDS and NN respectively for the original frame \ref{fig:missa2} taken from the 'missa' video. Another example (cropped to fit) can be seen in \ref{fig:missa4} \textcolor{red}{I will add tags to each image to show the method}. \ref{fig:ocean1} (VGPDS) and \ref{fig:ocean2} (NN) depict the reconstruction achieved for a frame of the 'ocean' dataset. 
Finally, we demonstrate the ability of the model to automatically select the latent dimensionality by showing the initial lengthscales (fig: \ref{fig:scalesDogInit}) of the ARD kernel and the values obtained after training (fig: \ref{fig:scalesDogOpt}) on the 'dog' dataset.
}
}
\label{fig:video1}
\end{figure}

\par As a second task, we used our generative model to create new samples and effectively generate a new video sequence. This is possible when the training examples are at least approximately periodic in nature. The model was trained on 60 frames (time-stamps $[t_1, t_{60}]$) of the aforementioned 'dog' video and generated the frames which correspond to the next 40 time points in the future. The only input given for the test phase was, thus, the test time vector $[t_{61}, t_{100}]$. The challenge is not only to generate a non-blurry video, but also to resume the motion while performing the transition from the training to the test frames. Indeed, the frame in $t_{61}$ is smoothly following the one in $t_{60}$ and the overall result achieved is quite realistic, as can be seen in figure \ref{fig:dog}. 


\begin{figure}[ht]
\begin{center}
\subfigure[]{
	\includegraphics[width=0.31\textwidth]{images/dog/generation_lastOfTraining}
	\label{fig:dogTrain}
}
\subfigure[]{
	\includegraphics[width=0.31\textwidth]{images/dog/generation_firstOfTest}
	\label{fig:dogTest1}
}
\subfigure[]{
	\includegraphics[width=0.31\textwidth]{images/dog/generation_test14}
	\label{fig:dogTest2}
}
\end{center}
\caption{ \small{
The last frame of the training video \ref{fig:dogTrain} is smoothly followed by the first frame \ref{fig:dogTest1} of the generated video. A subsequent generated frame can be seen in \ref{fig:dogTest2}}.}
\label{fig:dog}
\end{figure}


\section{Discussion and future work}

We have introduced a fully Bayesian approach for modelling dynamical
systems through probabilistic nonlinear dimensionality
reduction. Marginalizing the latent space and reconstructing data
using Gaussian processes results in a very generic model for capturing
complex, non-linear correlations even in very high dimensional data,
without having to perform any data preprocessing or exhaustive search
for defining the model's structure and parameters.


%\par Currently, the method is not application oriented but its effectiveness has been demonstrated in two tasks; firstly, in modelling human motion capture data and, secondly, in reconstructing and generating raw and very high dimensional video sequences. However, the approach seems promising for application to more specific areas, such as vision (e.g. tracking tasks) and finance, especially if we consider more sophisticated kernel functions to be used for the dynamics model.



\par Our method's effectiveness has been demonstrated in two tasks;
firstly, in modelling human motion capture data and, secondly, in
reconstructing and generating raw, very high dimensional video
sequences. A promising future direction to follow would be to enhance
our formulation with domain-specific knowledge encoded, for example,
in more sophisticated covariance functions or in the way that data are
being preprocessed. Thus, we can obtain application-oriented methods
to be used for tasks in areas such as vision and finance.

% \par More importantly, future work will also address some of the
% current limitations of our approach. Firstly, the algorithm can easily
% be extended so as to account for heteroscedastic noise as well as for
% the different intrinsic variances per data dimension (e.g. as in
% \cite{SGPLVM}). Furthermore, we can obtain an auto-regressive version
% of our method by modeling the dynamics using a prior which employs
% the Markov property. Given the above, we can also consider working
% towards a non-linear version of Factor Analysis.  (relation to Kalman
% filters??)







%\bibliographystyle{apalike}
\bibliographystyle{ieeetr}
\renewcommand*{\refname}{\begin{normalsize}References\end{normalsize}}
\bibliography{paper}
%\singlespace
%\bibliography{paper}












%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\section{Headings: first level}
%\label{headings}
%
%First level headings are lower case (except for first word and proper nouns),
%flush left, bold and in point size 12. One line space before the first level
%heading and 1/2~line space after the first level heading.
%
%\subsection{Headings: second level}
%
%Second level headings are lower case (except for first word and proper nouns),
%flush left, bold and in point size 10. One line space before the second level
%heading and 1/2~line space after the second level heading.
%
%\subsubsection{Headings: third level}
%
%Third level headings are lower case (except for first word and proper nouns),
%flush left, bold and in point size 10. One line space before the third level
%heading and 1/2~line space after the third level heading.
%\begin{verbatim}
%\newcommand{\RR}{I\!\!R} %real numbers
%\newcommand{\Nat}{I\!\!N} %natural numbers 
%\newcommand{\CC}{I\!\!\!\!C} %complex numbers
%\end{verbatim}
%
%\section{Citations, figures, tables, references}
%\label{others}
%
%These instructions apply to everyone, regardless of the formatter being used.
%
%\subsection{Citations within the text}
%
%Citations within the text should be numbered consecutively. The corresponding
%number is to appear enclosed in square brackets, such as [1] or [2]-[5]. The
%corresponding references are to be listed in the same order at the end of the
%paper, in the \textbf{References} section. (Note: the standard
%\textsc{Bib\TeX} style \texttt{unsrt} produces this.) As to the format of the
%references themselves, any style is acceptable as long as it is used
%consistently.
%
%
%\subsection{Figures}
%
%However, it is best for the
%figure captions and the paper body to make sense if the paper is printed
%either in black/white or in color.
%\begin{figure}[h]
%\begin{center}
%%\framebox[4.0in]{$\;$}
%\fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%\end{center}
%\caption{Sample figure caption.}
%\end{figure}
%
%\subsection{Tables}
%
%All tables must be centered, neat, clean and legible. Do not use hand-drawn
%tables. The table number and title always appear before the table. See
%Table~\ref{sample-table}.
%
%Place one line space before the table title, one line space after the table
%title, and one line space after the table. The table title must be lower case
%(except for first word and proper nouns); tables are numbered consecutively.
%
%\begin{table}[t]
%\caption{Sample table title}
%\label{sample-table}
%\begin{center}
%\begin{tabular}{ll}
%\multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
%\\ \hline \\
%Dendrite         &Input terminal \\
%Axon             &Output terminal \\
%Soma             &Cell body (contains cell nucleus) \\
%\end{tabular}
%\end{center}
%\end{table}
%
%
%
%\subsection{Margins in LaTeX}
% 
%Most of the margin problems come from figures positioned by hand using
%\verb+\special+ or other commands. We suggest using the command
%\verb+\includegraphics+
%from the graphicx package. Always specify the figure width as a multiple of
%the line width as in the example below using .eps graphics
%\begin{verbatim}
%   \usepackage[dvips]{graphicx} ... 
%   \includegraphics[width=0.8\linewidth]{myfile.eps} 
%\end{verbatim}
%or % Apr 2009 addition
%\begin{verbatim}
%   \usepackage[pdftex]{graphicx} ... 
%   \includegraphics[width=0.8\linewidth]{myfile.pdf} 
%\end{verbatim}
%for .pdf graphics. 
%See section 4.4 in the graphics bundle documentation (http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps) 
% 
%A number of width problems arise when LaTeX cannot properly hyphenate a
%line. Please give LaTeX hyphenation hints using the \verb+\-+ command.
%
%
%\subsubsection*{Acknowledgments}
%
%Use unnumbered third level headings for the acknowledgments. All
%acknowledgments go at the end of the paper. Do not include 
%acknowledgments in the anonymized submission, only in the 
%final paper. 
%
%\subsubsection*{References}
%
%References follow the acknowledgments. Use unnumbered third level heading for
%the references. Any choice of citation style is acceptable as long as you are
%consistent. It is permissible to reduce the font size to `small' (9-point) 
%when listing the references. {\bf Remember that this year you can use
%a ninth page as long as it contains \emph{only} cited references.}
%
%\small{
%[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
%for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
%and T.K. Leen (eds.), {\it Advances in Neural Information Processing
%Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.
%
%[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
%Realistic Neural Models with the GEneral NEural SImulation System.}
%New York: TELOS/Springer-Verlag.
%
%[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
%and recall at excitatory recurrent synapses and cholinergic modulation
%in rat hippocampal region CA3. {\it Journal of Neuroscience}
%{\bf 15}(7):5249-5262.
%}


\include{supplementary}
\end{document}
